{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38a04b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f60158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stopwords & Preprocessing ---\n",
    "georgian_stopwords = [\n",
    "    \"და\", \"ან\", \"არის\", \"რომ\", \"ეს\", \"ამ\", \"იმ\", \"თუ\", \"არ\", \"ვერ\", \n",
    "    \"მე\", \"შენ\", \"ის\", \"ჩვენ\", \"თქვენ\", \"მათ\", \"ისინი\", \"რა\", \"როგორც\", \n",
    "    \"მისი\", \"ჩემი\", \"შენი\", \"მისი\", \"ჩვენი\", \"თქვენი\", \"მათი\", \n",
    "    \"იყო\", \"იქნება\", \"თვის\", \"არიან\", \"მქონე\", \"აქვს\", \"იგი\",\n",
    "    \"ასევე\", \"უნდა\", \"კიდევ\", \"ყველა\", \"ერთი\", \"ორი\", \"სხვა\",\n",
    "    \"რომელიც\", \"რომელსაც\", \"როგორ\", \"სადაც\", \"როდესაც\", \"ის\", \"რაც\", \"მიერ\"\n",
    "]\n",
    "\n",
    "def georgian_stemmer(word):\n",
    "    \"\"\"Remove common Georgian suffixes to normalize words.\"\"\"\n",
    "    suffixes = ['ება', 'ების', 'მა', 'ში', 'ზე', 'თან', 'ით', 'დან', 'ზეა', 'ობით', 'შია']\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Normalize and clean Georgian text: lowercase, remove HTML/punctuation, tokenize, stem.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'</?[^>]+>', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        if word not in georgian_stopwords and len(word) > 2:\n",
    "            stemmed = georgian_stemmer(word)\n",
    "            processed_words.append(stemmed)\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "def extract_sentences_around_keyword(text, keyword, window_size=3):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    indices = [i for i, s in enumerate(sentences) if keyword.lower() in s.lower()]\n",
    "    contexts = []\n",
    "    for idx in indices:\n",
    "        start = max(0, idx - window_size)\n",
    "        end = min(len(sentences), idx + window_size + 1)\n",
    "        contexts.append(' '.join(sentences[start:end]))\n",
    "    return ' '.join(contexts)\n",
    "\n",
    "def remove_keyword_from_text(text, keyword):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    escaped_keyword = re.escape(keyword)\n",
    "    text = re.sub(escaped_keyword, '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --- Data Loading ---\n",
    "def load_and_combine_parquet_files(file_pattern, keyword, window_size=3, remove_keyword=True):\n",
    "    \"\"\"Load parquet files, extract contexts around keyword, optionally remove keyword.\"\"\"\n",
    "    all_files = sorted(glob.glob(file_pattern))\n",
    "    if not all_files:\n",
    "        raise ValueError(f\"No files match pattern: {file_pattern}\")\n",
    "    dfs = []\n",
    "    for file_path in all_files:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        if 'full_text' not in df.columns:\n",
    "            df['full_text'] = df['doc_content'] if 'doc_content' in df.columns else \"\"\n",
    "        filtered_df = df[df['full_text'].astype(str).str.contains(keyword, na=False)].copy()\n",
    "        if not filtered_df.empty:\n",
    "            filtered_df['context_text'] = filtered_df['full_text'].apply(\n",
    "                lambda t: extract_sentences_around_keyword(t, keyword, window_size)\n",
    "            )\n",
    "            if remove_keyword:\n",
    "                filtered_df['context_text'] = filtered_df['context_text'].apply(\n",
    "                    lambda t: remove_keyword_from_text(t, keyword)\n",
    "                )\n",
    "            dfs.append(filtered_df)\n",
    "    if not dfs:\n",
    "        raise ValueError(f\"No data found containing keyword: {keyword}\")\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    combined_df = combined_df.dropna(subset=['context_text'])\n",
    "    combined_df['context_text'] = combined_df['context_text'].astype(str)\n",
    "    combined_df = combined_df[combined_df['context_text'] != '']\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def check_random_samples_from_list(text_list, keyword, n_samples=1000):\n",
    "    \"\"\"Select random samples from a list of texts and check if they contain the keyword.\"\"\"\n",
    "    import random\n",
    "    \n",
    "    # Adjust sample size if list is smaller than requested samples\n",
    "    sample_size = min(n_samples, len(text_list))\n",
    "    \n",
    "    if sample_size < n_samples:\n",
    "        print(f\"Warning: List has only {len(text_list)} items. Using all items instead of {n_samples}.\")\n",
    "    \n",
    "    # Take random sample\n",
    "    if sample_size < len(text_list):\n",
    "        random.seed(42)\n",
    "        sample_texts = random.sample(text_list, sample_size)\n",
    "    else:\n",
    "        sample_texts = text_list.copy()\n",
    "    \n",
    "    samples_with_keyword = []\n",
    "    samples_without_keyword = []\n",
    "    \n",
    "    for text in sample_texts:\n",
    "        if keyword.lower() in text.lower():\n",
    "            samples_with_keyword.append(text)\n",
    "        else:\n",
    "            samples_without_keyword.append(text)\n",
    "    \n",
    "    count_with_keyword = len(samples_with_keyword)\n",
    "    percent_with_keyword = (count_with_keyword / sample_size) * 100\n",
    "    \n",
    "    print(f\"Random sample results:\")\n",
    "    print(f\"- Sample size: {sample_size}\")\n",
    "    print(f\"- Texts containing keyword '{keyword}': {count_with_keyword} ({percent_with_keyword:.2f}%)\")\n",
    "    \n",
    "    return percent_with_keyword, samples_with_keyword, samples_without_keyword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cde9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = \"corpora/corpus/split_*.parquet\"  # This will match all files starting with \"split_\" and ending with \".parquet\"\n",
    "keyword = \"ხელოვნური ინტელექტი\"\n",
    "window_size = 1  # Adjust as needed\n",
    "\n",
    "# Load and combine the data\n",
    "df = load_and_combine_parquet_files(file_pattern, keyword, window_size)\n",
    "\n",
    "\n",
    "# Applying preprocessing\n",
    "df['context_text'] = df['context_text'].apply(lambda text: remove_keyword_from_text(text, keyword))\n",
    "df['processed_text'] = df['context_text'].apply(preprocess_text)\n",
    "texts = df['processed_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3984752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk for later reuse\n",
    "df.to_parquet('data/processed_corpus.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d9a8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample results:\n",
      "- Sample size: 1000\n",
      "- Texts containing keyword 'ხელოვნური ინტელექტი': 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "percent_with_keyword, with_keyword, without_keyword = check_random_samples_from_list(texts, keyword)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
